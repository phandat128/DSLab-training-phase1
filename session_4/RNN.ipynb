{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "uXFK32dxYSOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJMLcAA_3bTI",
        "outputId": "e22b54da-8b7a-453a-9b02-c46f2ba5efe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_DOC_LENGTH = 500\n",
        "NUM_CLASSES = 20\n",
        "ROOT_PATH = '/content/drive/MyDrive/Colab Notebooks/'"
      ],
      "metadata": {
        "id": "XBupMgbk1Ajf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bk0H-4aFYJXA"
      },
      "outputs": [],
      "source": [
        "class RNN:\n",
        "    def __init__(self, vocab_size, embedding_size, lstm_size, batch_size):\n",
        "        self._vocab_size = vocab_size\n",
        "        self._embedding_size = embedding_size\n",
        "        self._lstm_size = lstm_size\n",
        "        self._batch_size = batch_size\n",
        "\n",
        "        self._data = tf.compat.v1.placeholder(tf.int32, shape=[batch_size, MAX_DOC_LENGTH])\n",
        "        self._labels = tf.compat.v1.placeholder(tf.int32, shape=[batch_size, ])\n",
        "        self._sentence_lengths = tf.compat.v1.placeholder(tf.int32, shape=[batch_size, ])\n",
        "        self._final_token = tf.compat.v1.placeholder(tf.int32, shape=[batch_size, ])\n",
        "\n",
        "    def embedding_layer(self, indices):\n",
        "        pretrained_vectors = [np.zeros(self._embedding_size)]\n",
        "        np.random.seed(2023)\n",
        "        for _ in range(self._vocab_size + 1):\n",
        "            pretrained_vectors.append(np.random.normal(loc=0., scale=1., size=self._embedding_size))\n",
        "\n",
        "        pretrained_vectors = np.array(pretrained_vectors)\n",
        "\n",
        "        self._embedding_matrix = tf.compat.v1.get_variable(\n",
        "            name='embedding',\n",
        "            shape=(self._vocab_size + 2, self._embedding_size),\n",
        "            initializer=tf.constant_initializer(pretrained_vectors)\n",
        "        )\n",
        "        return tf.nn.embedding_lookup(self._embedding_matrix, indices)\n",
        "\n",
        "    def lstm_layer(self, embeddings):\n",
        "        lstm_cell = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(self._lstm_size)\n",
        "        zero_state = tf.zeros(shape=(self._batch_size, self._lstm_size))\n",
        "        initial_state = tf.compat.v1.nn.rnn_cell.LSTMStateTuple(zero_state, zero_state)\n",
        "\n",
        "        lstm_inputs = tf.unstack(\n",
        "            tf.transpose(embeddings, perm=[1, 0, 2])\n",
        "        )\n",
        "        lstm_outputs, last_state = tf.compat.v1.nn.static_rnn(\n",
        "            cell=lstm_cell,\n",
        "            inputs=lstm_inputs,\n",
        "            initial_state=initial_state,\n",
        "            sequence_length=self._sentence_lengths\n",
        "        )\n",
        "\n",
        "        lstm_outputs = tf.unstack(\n",
        "            tf.transpose(lstm_outputs, perm=[1, 0, 2])\n",
        "        )\n",
        "        lstm_outputs = tf.concat(\n",
        "            lstm_outputs,\n",
        "            axis=0\n",
        "        ) # [num_doc * MAX_SENTENCE_LENGTH, lstm_size]\n",
        "\n",
        "        # mask = [num_doc * MAX_SENTENCE_LENGTH, ]\n",
        "        mask = tf.sequence_mask(\n",
        "            lengths=self._sentence_lengths,\n",
        "            maxlen=MAX_DOC_LENGTH,\n",
        "            dtype=tf.float32\n",
        "        )\n",
        "        mask = tf.concat(\n",
        "            tf.unstack(mask, axis=0),\n",
        "            axis=0\n",
        "        )\n",
        "        mask = tf.expand_dims(mask, -1)\n",
        "\n",
        "        lstm_outputs = mask * lstm_outputs\n",
        "        lstm_outputs_split = tf.split(lstm_outputs, num_or_size_splits=self._batch_size)\n",
        "        lstm_output_sum = tf.reduce_sum(lstm_outputs_split, axis=1) #[num_doc, lstm_size]\n",
        "        lstm_output_average = lstm_output_sum / tf.expand_dims(\n",
        "            input=tf.cast(self._sentence_lengths, tf.float32), axis=-1\n",
        "        )\n",
        "        return lstm_output_average\n",
        "\n",
        "    def build_graph(self):\n",
        "        embeddings = self.embedding_layer(self._data)\n",
        "        lstm_outputs = self.lstm_layer(embeddings)\n",
        "\n",
        "        weights = tf.compat.v1.get_variable(\n",
        "            name='final_layer_weights',\n",
        "            shape=(self._lstm_size, NUM_CLASSES),\n",
        "            initializer=tf.random_normal_initializer(seed=2023)\n",
        "        )\n",
        "\n",
        "        biases = tf.compat.v1.get_variable(\n",
        "            name='final_layer_biases',\n",
        "            shape=NUM_CLASSES,\n",
        "            initializer=tf.random_normal_initializer(seed=2023)\n",
        "        )\n",
        "\n",
        "        logits = tf.matmul(lstm_outputs, weights) + biases\n",
        "\n",
        "        labels_one_hot = tf.one_hot(\n",
        "            indices=self._labels,\n",
        "            depth=NUM_CLASSES,\n",
        "            dtype=tf.float32\n",
        "        )\n",
        "\n",
        "        loss = tf.nn.softmax_cross_entropy_with_logits(\n",
        "            labels=labels_one_hot,\n",
        "            logits=logits\n",
        "        )\n",
        "        loss = tf.reduce_mean(loss)\n",
        "\n",
        "        probs = tf.nn.softmax(logits)\n",
        "        predicted_labels = tf.argmax(probs, axis=1)\n",
        "        predicted_labels = tf.squeeze(predicted_labels)\n",
        "\n",
        "        return predicted_labels, loss\n",
        "\n",
        "    def trainer(self, loss, learning_rate):\n",
        "        train_op = tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(loss)\n",
        "        return train_op\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataReader:\n",
        "    def __init__(self, data_path, batch_size):\n",
        "        self._batch_size = batch_size\n",
        "        with open(data_path) as f:\n",
        "            d_lines = f.read().splitlines()\n",
        "\n",
        "        self._data = []\n",
        "        self._labels = []\n",
        "        self._sentence_lengths = []\n",
        "        self._final_tokens = []\n",
        "\n",
        "        for data_id, line in enumerate(d_lines):\n",
        "            features = line.split('<fff>')\n",
        "            label = int(features[0])\n",
        "            doc_id = int(features[1])\n",
        "            sentence_length = int(features[2])\n",
        "            tokens = features[3].split()\n",
        "\n",
        "            self._data.append(tokens)\n",
        "            self._labels.append(label)\n",
        "            self._sentence_lengths.append(sentence_length)\n",
        "            self._final_tokens.append(tokens[-1])\n",
        "\n",
        "        self._data = np.array(self._data)\n",
        "        self._labels = np.array(self._labels)\n",
        "        self._sentence_lengths = np.array(self._sentence_lengths)\n",
        "        self._final_tokens = np.array(self._final_tokens)\n",
        "\n",
        "        self._num_epoch = 0\n",
        "        self._batch_id = 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        start = self._batch_id * self._batch_size\n",
        "        end = start + self._batch_size\n",
        "        self._batch_id += 1\n",
        "\n",
        "        if end + self._batch_size > len(self._data):\n",
        "            self._num_epoch += 1\n",
        "            self._batch_id = 0\n",
        "            indices = list(range(len(self._data)))\n",
        "            np.random.shuffle(indices)\n",
        "            self._data = self._data[indices]\n",
        "            self._labels = self._labels[indices]\n",
        "            self._sentence_lenghts = self._sentence_lengths[indices]\n",
        "            self._final_tokens = self._final_tokens[indices]\n",
        "\n",
        "        return self._data[start:end], self._labels[start:end], \\\n",
        "            self._sentence_lengths[start:end], self._final_tokens[start:end]\n"
      ],
      "metadata": {
        "id": "dKuDVhFfYZ93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.compat.v1.disable_eager_execution()"
      ],
      "metadata": {
        "id": "xB2I1Uvf3qJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(ROOT_PATH + 'w2v/vocab-raw.txt', 'rb') as f:\n",
        "  vocab_size = len(f.read().splitlines())"
      ],
      "metadata": {
        "id": "5sWDJvoG4E8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.compat.v1.set_random_seed(2023)\n",
        "rnn = RNN(\n",
        "  vocab_size=vocab_size,\n",
        "  embedding_size=300,\n",
        "  lstm_size=50,\n",
        "  batch_size=50\n",
        ")\n",
        "predicted_labels, loss = rnn.build_graph()\n",
        "train_op = rnn.trainer(learning_rate=0.02, loss=loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FauxV9e64I4o",
        "outputId": "05f17e36-6919-4f05-86d8-21ba94b2fd7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-c40ed1d58afd>:29: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  lstm_cell = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(self._lstm_size)\n",
            "WARNING:tensorflow:From <ipython-input-4-c40ed1d58afd>:36: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/keras/layers/rnn/legacy_cells.py:797: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_reader = DataReader(\n",
        "    data_path=ROOT_PATH + 'w2v/20news-train-encoded.txt',\n",
        "    batch_size=50\n",
        ")\n",
        "\n",
        "test_data_reader = DataReader(\n",
        "    data_path=ROOT_PATH + 'w2v/20news-test-encoded.txt',\n",
        "    batch_size=50\n",
        ")"
      ],
      "metadata": {
        "id": "niy-mZcfrQKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.compat.v1.Session() as session:\n",
        "  step = 0\n",
        "  MAX_STEP = 2000\n",
        "  session.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "  while step < MAX_STEP:\n",
        "      next_train_batch = train_data_reader.next_batch()\n",
        "      train_data, train_labels, train_sentence_lengths, train_final_tokens = next_train_batch\n",
        "      predicted_labels_eval, loss_eval, _ = session.run(\n",
        "          [predicted_labels, loss, train_op],\n",
        "          feed_dict={\n",
        "              rnn._data: train_data,\n",
        "              rnn._labels: train_labels,\n",
        "              rnn._sentence_lengths: train_sentence_lengths,\n",
        "              rnn._final_token: train_final_tokens\n",
        "          }\n",
        "      )\n",
        "      step +=1\n",
        "      if step % 20 == 0: print('Step: {}. Loss: {}'.format(step, loss_eval))\n",
        "\n",
        "      if train_data_reader._batch_id == 0:\n",
        "          num_true_preds = 0\n",
        "          while True:\n",
        "              next_test_batch = test_data_reader.next_batch()\n",
        "              test_data, test_labels, test_sentence_lengths, test_final_tokens = next_test_batch\n",
        "\n",
        "              test_predict_labels_eval = session.run(\n",
        "                  predicted_labels,\n",
        "                  feed_dict={\n",
        "                      rnn._data: test_data,\n",
        "                      rnn._labels: test_labels,\n",
        "                      rnn._sentence_lengths: test_sentence_lengths,\n",
        "                      rnn._final_token: test_final_tokens\n",
        "                  }\n",
        "              )\n",
        "              matches = np.equal(test_predict_labels_eval, test_labels)\n",
        "              num_true_preds += np.sum(matches.astype(float))\n",
        "\n",
        "              if test_data_reader._batch_id == 0: break\n",
        "\n",
        "          print('Epoch: ', train_data_reader._num_epoch)\n",
        "          print('Accuracy on test data: ', num_true_preds * 1. / len(test_data_reader._data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx4xNbE70wcu",
        "outputId": "daeb7a11-0bfe-40a5-f990-656e0833f47d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 20. Loss: 0.5579636096954346\n",
            "Step: 40. Loss: 2.019996404647827\n",
            "Step: 60. Loss: 4.636195659637451\n",
            "Step: 80. Loss: 0.3499857783317566\n",
            "Step: 100. Loss: 1.3557624816894531\n",
            "Step: 120. Loss: 3.2398064136505127\n",
            "Step: 140. Loss: 1.7121505737304688\n",
            "Step: 160. Loss: 1.292830467224121\n",
            "Step: 180. Loss: 3.762752056121826\n",
            "Step: 200. Loss: 5.480381965637207\n",
            "Step: 220. Loss: 5.070803642272949\n",
            "Epoch:  1\n",
            "Accuracy on test data:  0.04633563462559745\n",
            "Step: 240. Loss: 3.547686815261841\n",
            "Step: 260. Loss: 3.338289737701416\n",
            "Step: 280. Loss: 2.9535794258117676\n",
            "Step: 300. Loss: 2.756516695022583\n",
            "Step: 320. Loss: 2.6970083713531494\n",
            "Step: 340. Loss: 2.6448025703430176\n",
            "Step: 360. Loss: 2.677276849746704\n",
            "Step: 380. Loss: 2.6131949424743652\n",
            "Step: 400. Loss: 2.481698989868164\n",
            "Step: 420. Loss: 2.5577971935272217\n",
            "Step: 440. Loss: 2.418139696121216\n",
            "Epoch:  2\n",
            "Accuracy on test data:  0.34466277217206587\n",
            "Step: 460. Loss: 2.3391494750976562\n",
            "Step: 480. Loss: 2.2968873977661133\n",
            "Step: 500. Loss: 2.144426107406616\n",
            "Step: 520. Loss: 2.097404718399048\n",
            "Step: 540. Loss: 2.1600983142852783\n",
            "Step: 560. Loss: 2.1833314895629883\n",
            "Step: 580. Loss: 2.184795618057251\n",
            "Step: 600. Loss: 1.8133491277694702\n",
            "Step: 620. Loss: 1.921254277229309\n",
            "Step: 640. Loss: 2.013888359069824\n",
            "Step: 660. Loss: 2.102766990661621\n",
            "Epoch:  3\n",
            "Accuracy on test data:  0.4548592671269251\n",
            "Step: 680. Loss: 1.926863670349121\n",
            "Step: 700. Loss: 1.7344638109207153\n",
            "Step: 720. Loss: 1.4673080444335938\n",
            "Step: 740. Loss: 1.6608786582946777\n",
            "Step: 760. Loss: 1.9067540168762207\n",
            "Step: 780. Loss: 1.5070796012878418\n",
            "Step: 800. Loss: 1.472965121269226\n",
            "Step: 820. Loss: 1.5511317253112793\n",
            "Step: 840. Loss: 1.6374033689498901\n",
            "Step: 860. Loss: 1.7602150440216064\n",
            "Step: 880. Loss: 1.7822480201721191\n",
            "Step: 900. Loss: 1.5770782232284546\n",
            "Epoch:  4\n",
            "Accuracy on test data:  0.5313329792883696\n",
            "Step: 920. Loss: 1.2847771644592285\n",
            "Step: 940. Loss: 1.371331810951233\n",
            "Step: 960. Loss: 1.1497673988342285\n",
            "Step: 980. Loss: 1.1449192762374878\n",
            "Step: 1000. Loss: 1.2607280015945435\n",
            "Step: 1020. Loss: 1.3118966817855835\n",
            "Step: 1040. Loss: 1.3827203512191772\n",
            "Step: 1060. Loss: 1.4712274074554443\n",
            "Step: 1080. Loss: 1.582614541053772\n",
            "Step: 1100. Loss: 1.4203660488128662\n",
            "Step: 1120. Loss: 1.3106133937835693\n",
            "Epoch:  5\n",
            "Accuracy on test data:  0.5744822092405736\n",
            "Step: 1140. Loss: 0.9617838859558105\n",
            "Step: 1160. Loss: 0.9941194653511047\n",
            "Step: 1180. Loss: 0.9326062202453613\n",
            "Step: 1200. Loss: 0.9530697464942932\n",
            "Step: 1220. Loss: 0.7910123467445374\n",
            "Step: 1240. Loss: 1.0470448732376099\n",
            "Step: 1260. Loss: 1.3460044860839844\n",
            "Step: 1280. Loss: 0.834115207195282\n",
            "Step: 1300. Loss: 1.063470482826233\n",
            "Step: 1320. Loss: 1.305713176727295\n",
            "Step: 1340. Loss: 1.1192888021469116\n",
            "Epoch:  6\n",
            "Accuracy on test data:  0.6167020711630377\n",
            "Step: 1360. Loss: 1.0757218599319458\n",
            "Step: 1380. Loss: 0.7968280911445618\n",
            "Step: 1400. Loss: 0.8417897820472717\n",
            "Step: 1420. Loss: 0.7386477589607239\n",
            "Step: 1440. Loss: 0.809532105922699\n",
            "Step: 1460. Loss: 0.7598521709442139\n",
            "Step: 1480. Loss: 0.9772999286651611\n",
            "Step: 1500. Loss: 0.7320038676261902\n",
            "Step: 1520. Loss: 0.9226251244544983\n",
            "Step: 1540. Loss: 0.9913299679756165\n",
            "Step: 1560. Loss: 0.9224404692649841\n",
            "Step: 1580. Loss: 0.8078603148460388\n",
            "Epoch:  7\n",
            "Accuracy on test data:  0.6394052044609665\n",
            "Step: 1600. Loss: 0.6410266757011414\n",
            "Step: 1620. Loss: 0.5848371386528015\n",
            "Step: 1640. Loss: 0.619551420211792\n",
            "Step: 1660. Loss: 0.5298943519592285\n",
            "Step: 1680. Loss: 0.7550067901611328\n",
            "Step: 1700. Loss: 0.7252182960510254\n",
            "Step: 1720. Loss: 0.8480159044265747\n",
            "Step: 1740. Loss: 0.6909385919570923\n",
            "Step: 1760. Loss: 0.778968334197998\n",
            "Step: 1780. Loss: 1.0524342060089111\n",
            "Step: 1800. Loss: 0.5398435592651367\n",
            "Epoch:  8\n",
            "Accuracy on test data:  0.6567976633032395\n",
            "Step: 1820. Loss: 0.49432289600372314\n",
            "Step: 1840. Loss: 0.27186718583106995\n",
            "Step: 1860. Loss: 0.4381883144378662\n",
            "Step: 1880. Loss: 0.3951728940010071\n",
            "Step: 1900. Loss: 0.49856293201446533\n",
            "Step: 1920. Loss: 0.566241979598999\n",
            "Step: 1940. Loss: 0.6960274577140808\n",
            "Step: 1960. Loss: 0.47411102056503296\n",
            "Step: 1980. Loss: 0.6429934501647949\n",
            "Step: 2000. Loss: 0.6648823022842407\n"
          ]
        }
      ]
    }
  ]
}