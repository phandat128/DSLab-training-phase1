{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eTD-g-7pEfkP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TcIxNMSgqHV",
        "outputId": "241de737-2508-4baf-bffc-d898beb66412"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_PATH = '/content/drive/MyDrive/Colab Notebooks/'"
      ],
      "metadata": {
        "id": "V-BAWxwlf_6K"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        self._vocab_size = vocab_size\n",
        "        self._hidden_size = hidden_size\n",
        "\n",
        "    def build_graph(self):\n",
        "        NUM_CLASSES = 20\n",
        "        self._X = tf.compat.v1.placeholder(tf.float32, shape=[None, self._vocab_size])\n",
        "        self._real_Y = tf.compat.v1.placeholder(tf.int32, shape=[None, ])\n",
        "\n",
        "        weights_1 = tf.compat.v1.get_variable(\n",
        "            name='weights_input_hidden',\n",
        "            shape=(self._vocab_size, self._hidden_size),\n",
        "            initializer=tf.random_normal_initializer(seed=2023),\n",
        "        )\n",
        "\n",
        "        biases_1 = tf.compat.v1.get_variable(\n",
        "            name='biases_input_hidden',\n",
        "            shape=self._hidden_size,\n",
        "            initializer=tf.random_normal_initializer(seed=2023),\n",
        "        )\n",
        "\n",
        "        weights_2 = tf.compat.v1.get_variable(\n",
        "            name='weights_hidden_output',\n",
        "            shape=(self._hidden_size, NUM_CLASSES),\n",
        "            initializer=tf.random_normal_initializer(seed=2023),\n",
        "        )\n",
        "\n",
        "        biases_2 = tf.compat.v1.get_variable(\n",
        "            name='biases_hidden_output',\n",
        "            shape=NUM_CLASSES,\n",
        "            initializer=tf.random_normal_initializer(seed=2023),\n",
        "        )\n",
        "\n",
        "        hidden = tf.matmul(self._X, weights_1) + biases_1\n",
        "        hidden = tf.sigmoid(hidden)\n",
        "        logits = tf.matmul(hidden, weights_2) + biases_2\n",
        "\n",
        "        labels_one_hot = tf.one_hot(indices=self._real_Y, depth=NUM_CLASSES, dtype=tf.float32)\n",
        "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot, logits=logits)\n",
        "        loss = tf.reduce_mean(loss)\n",
        "\n",
        "        probs = tf.nn.softmax(logits)\n",
        "        predicted_labels = tf.argmax(probs, axis=1)\n",
        "        predicted_labels = tf.squeeze(predicted_labels)\n",
        "\n",
        "        return predicted_labels, loss\n",
        "\n",
        "    def trainer(self, loss, learning_rate):\n",
        "        train_op = tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(loss)\n",
        "        return train_op"
      ],
      "metadata": {
        "id": "DJWNRHlQ1uzc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataReader:\n",
        "    def __init__(self, data_path, batch_size, vocab_size, purpose):\n",
        "        assert(purpose in ('train', 'test'))            \n",
        "        self._batch_size = batch_size\n",
        "        with open(data_path) as f:\n",
        "            d_lines = f.read().splitlines()\n",
        "\n",
        "        self._data = []\n",
        "        self._labels = []\n",
        "\n",
        "        for data_id, line in enumerate(d_lines):\n",
        "            vector = [(0.0) for _ in range(vocab_size)]\n",
        "            features = line.split('<fff>')\n",
        "            label = int(features[0])\n",
        "            # doc_id = int(features[1])\n",
        "            tokens = features[2].split()\n",
        "            for token in tokens:\n",
        "                index = int(token.split(':')[0])\n",
        "                value = float(token.split(':')[1])\n",
        "                vector[index] = value\n",
        "            self._data.append(vector)\n",
        "            self._labels.append(label)\n",
        "\n",
        "        indices = list(range(len(self._data)))\n",
        "        random.seed(128)\n",
        "        random.shuffle(self._data)\n",
        "        random.seed(128)\n",
        "        random.shuffle(self._labels)\n",
        "        _splitter = int(0.8 * len(self._data))\n",
        "        if purpose == 'train': \n",
        "            self._data = np.array(self._data[:_splitter])\n",
        "            self._labels = np.array(self._labels[:_splitter])\n",
        "        else:\n",
        "            self._data = np.array(self._data[_splitter:])\n",
        "            self._labels = np.array(self._labels[_splitter:])\n",
        "\n",
        "        self._num_epoch = 0\n",
        "        self._batch_id = 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        start = self._batch_id * self._batch_size\n",
        "        end = start + self._batch_size\n",
        "        self._batch_id += 1\n",
        "\n",
        "        if end + self._batch_size > len(self._data):\n",
        "            end = len(self._data)\n",
        "            self._num_epoch += 1\n",
        "            self._batch_id = 0\n",
        "            indices = list(range(len(self._data)))\n",
        "            random.seed(2023)\n",
        "            random.shuffle(indices)\n",
        "            self._data = self._data[indices]\n",
        "            self._labels = self._labels[indices]\n",
        "\n",
        "        return self._data[start:end], self._labels[start:end]"
      ],
      "metadata": {
        "id": "cU3CPLrN13jW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(ROOT_PATH + 'test-word_idfs.txt', 'rb') as f:\n",
        "  # print(f)\n",
        "  # print(len(f.read().splitlines()))\n",
        "  vocab_size = len(f.read().splitlines())\n",
        "  print(vocab_size)"
      ],
      "metadata": {
        "id": "7-IC44--gaMm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ac74b17-e6d6-486e-dc78-ff5835beb162"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_parameters(name, value, epoch):\n",
        "    filename = name.replace(':', '-colon') + '-epoch-{}.txt'.format(epoch)\n",
        "    if len(value.shape) == 1:  # is a list\n",
        "        string_form = ','.join([str(number) for number in value])\n",
        "    else:\n",
        "        string_form = '\\n'.join([','.join([str(number) for number in value[row]])\n",
        "                                 for row in range(value.shape[0])])\n",
        "\n",
        "    with open(ROOT_PATH + 'saved-params/' + filename, 'w') as f:\n",
        "        f.write(string_form)"
      ],
      "metadata": {
        "id": "ZpN2lczK17dn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset():\n",
        "    _test_data_reader = DataReader(\n",
        "        data_path=ROOT_PATH + 'test-tf-idf.txt',\n",
        "        batch_size=50,\n",
        "        vocab_size=vocab_size,\n",
        "        purpose='test'\n",
        "    )\n",
        "    _train_data_reader = DataReader(\n",
        "        data_path=ROOT_PATH + 'test-tf-idf.txt',\n",
        "        batch_size=50,\n",
        "        vocab_size=vocab_size,\n",
        "        purpose='train'\n",
        "    ) \n",
        "    return _train_data_reader, _test_data_reader"
      ],
      "metadata": {
        "id": "gHV4iYQl197O"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.compat.v1.disable_eager_execution()"
      ],
      "metadata": {
        "id": "OvSSjpH1i7Xt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = MLP(\n",
        "        vocab_size=vocab_size,\n",
        "        hidden_size=50,\n",
        "    )\n",
        "predicted_labels, loss = mlp.build_graph()\n",
        "train_op = mlp.trainer(loss=loss, learning_rate=0.1)"
      ],
      "metadata": {
        "id": "UTUj13gIfY4D"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_reader, test_data_reader = load_dataset()"
      ],
      "metadata": {
        "id": "D5QZEorZ5Pt_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.compat.v1.Session() as session:\n",
        "    step = 0\n",
        "    MAX_STEP = 3000\n",
        "\n",
        "    session.run(tf.compat.v1.global_variables_initializer())\n",
        "    while step < MAX_STEP:\n",
        "        train_data, train_labels = train_data_reader.next_batch()\n",
        "        plabels_eval, loss_eval, _ = session.run(\n",
        "            [predicted_labels, loss, train_op],\n",
        "            feed_dict = {\n",
        "                mlp._X: train_data,\n",
        "                mlp._real_Y: train_labels\n",
        "            }\n",
        "        )\n",
        "        step += 1\n",
        "        if step % 100 == 0: print('step: {}, loss: {}'.format(step, loss_eval))\n",
        "    trainable_variables = tf.compat.v1.trainable_variables()\n",
        "    for variable in trainable_variables:\n",
        "      save_parameters(\n",
        "          name=variable.name,\n",
        "          value=variable.eval(),\n",
        "          epoch=train_data_reader._num_epoch\n",
        "      )\n",
        "    "
      ],
      "metadata": {
        "id": "uoACCsS4i4Gu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0c4a1bf-3c33-4f87-99ca-f6faa99f39e5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 100, loss: 0.3776494562625885\n",
            "step: 200, loss: 0.01657969318330288\n",
            "step: 300, loss: 0.0012482856400310993\n",
            "step: 400, loss: 0.0007727577467449009\n",
            "step: 500, loss: 0.0005408198921941221\n",
            "step: 600, loss: 0.0001748702343320474\n",
            "step: 700, loss: 0.00011850064765894786\n",
            "step: 800, loss: 0.0001043211595970206\n",
            "step: 900, loss: 0.0001256909454241395\n",
            "step: 1000, loss: 8.385296678170562e-05\n",
            "step: 1100, loss: 6.866596959298477e-05\n",
            "step: 1200, loss: 9.408150799572468e-05\n",
            "step: 1300, loss: 3.583434590836987e-05\n",
            "step: 1400, loss: 6.453356036217883e-05\n",
            "step: 1500, loss: 3.231943992432207e-05\n",
            "step: 1600, loss: 4.201159754302353e-05\n",
            "step: 1700, loss: 6.0002730606356636e-05\n",
            "step: 1800, loss: 0.01597045734524727\n",
            "step: 1900, loss: 9.061457240022719e-05\n",
            "step: 2000, loss: 1.496758977737045e-05\n",
            "step: 2100, loss: 3.6726269172504544e-05\n",
            "step: 2200, loss: 4.82721661683172e-05\n",
            "step: 2300, loss: 1.536058698548004e-05\n",
            "step: 2400, loss: 1.4307875972008333e-05\n",
            "step: 2500, loss: 2.7775608032243326e-05\n",
            "step: 2600, loss: 2.633289113873616e-05\n",
            "step: 2700, loss: 5.910733307246119e-05\n",
            "step: 2800, loss: 1.2149419490015134e-05\n",
            "step: 2900, loss: 2.9149014153517783e-05\n",
            "step: 3000, loss: 1.7507018128526397e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def restore_parameters(name, epoch):\n",
        "    filename = name.replace(':', '-colon') + '-epoch-{}.txt'.format(epoch)\n",
        "    with open(ROOT_PATH + 'saved-params/' + filename) as f:\n",
        "        lines = f.read().splitlines()\n",
        "    if len(lines) == 1: # is a vector\n",
        "        value = [float(number) for number in lines[0].split(',')]\n",
        "    else:\n",
        "        value = [[float(number) for number in lines[row].split(',')]\n",
        "                 for row in range(len(lines))]\n",
        "    return value"
      ],
      "metadata": {
        "id": "8N543gNr2kNY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.compat.v1.Session() as session:\n",
        "    _num_epoch = train_data_reader._num_epoch\n",
        "\n",
        "    trainable_variables = tf.compat.v1.trainable_variables()\n",
        "    for variable in trainable_variables:\n",
        "        _saved_value = restore_parameters(variable.name, _num_epoch)\n",
        "        # print(_saved_value)\n",
        "        _assign_op = variable.assign(_saved_value)\n",
        "        session.run(_assign_op)\n",
        "\n",
        "    num_true_preds = 0\n",
        "    while True:\n",
        "        test_data, test_labels = test_data_reader.next_batch()\n",
        "        # print(test_labels)\n",
        "        test_pred_labels_eval = session.run(\n",
        "            predicted_labels,\n",
        "            feed_dict = {\n",
        "                mlp._X: test_data,\n",
        "                mlp._real_Y: test_labels\n",
        "            }\n",
        "        )\n",
        "        matches = np.equal(test_pred_labels_eval, test_labels)\n",
        "        num_true_preds += np.sum(matches.astype(float))\n",
        "\n",
        "        if test_data_reader._batch_id == 0: break\n",
        "    print('Accuracy on test data: ', num_true_preds / len(test_data_reader._data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NONBenXX5-BR",
        "outputId": "d64d8d6c-4b39-41bf-bd97-f0c97c4816e8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test data:  0.9051094890510949\n"
          ]
        }
      ]
    }
  ]
}